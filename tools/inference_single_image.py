# ------------------------------------------------------------------------------
# Single Image Inference Script for Litchi Segmentation
# å•å¼ å›¾ç‰‡æ¨ç†è„šæœ¬ - è”æåˆ†å‰²
# ------------------------------------------------------------------------------

import argparse
import os
import sys
import time
import cv2
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.backends.cudnn as cudnn
import torch.nn.functional as F

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import _init_paths
import models
from config import config
from config import update_config

class LitchiInference:
    def __init__(self, config_file, model_file):
        """
        åˆå§‹åŒ–æ¨ç†å™¨
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„
            model_file: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
        """
        # æ›´æ–°é…ç½®
        update_config(config, argparse.Namespace(cfg=config_file, opts=[]))
        
        # è®¾ç½®CUDNN
        cudnn.benchmark = config.CUDNN.BENCHMARK
        cudnn.deterministic = config.CUDNN.DETERMINISTIC
        cudnn.enabled = config.CUDNN.ENABLED
        
        # æ„å»ºæ¨¡å‹
        if torch.__version__.startswith('1'):
            module = eval('models.' + config.MODEL.NAME)
            module.BatchNorm2d_class = module.BatchNorm2d = torch.nn.BatchNorm2d
        
        self.model = eval('models.' + config.MODEL.NAME + '.get_seg_model')(config)
        
        # åŠ è½½æ¨¡å‹æƒé‡
        self.load_model(model_file)
        
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = self.model.to(self.device)
        self.model.eval()
        
        # å›¾åƒé¢„å¤„ç†å‚æ•°
        self.mean = np.array([0.485, 0.456, 0.406])
        self.std = np.array([0.229, 0.224, 0.225])
        self.input_size = (config.TEST.IMAGE_SIZE[1], config.TEST.IMAGE_SIZE[0])  # (height, width)
        
        # ç±»åˆ«ä¿¡æ¯
        self.class_names = ['Background', 'Litchi', 'Litchi_Stem']
        self.class_colors = [
            [0, 0, 0],      # èƒŒæ™¯ - é»‘è‰²
            [255, 0, 0],    # è”æ - çº¢è‰²
            [0, 255, 0]     # è”ææ¢— - ç»¿è‰²
        ]
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print(f"ğŸ“± è®¾å¤‡: {self.device}")
        print(f"ğŸ–¼ï¸ è¾“å…¥å°ºå¯¸: {self.input_size}")
        print(f"ğŸ·ï¸ ç±»åˆ«: {self.class_names}")
    
    def load_model(self, model_file):
        """åŠ è½½æ¨¡å‹æƒé‡"""
        if not os.path.exists(model_file):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_file}")
        
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_file}")
        checkpoint = torch.load(model_file, map_location='cpu')
        
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        # å¤„ç†DataParallelä¿å­˜çš„æ¨¡å‹
        if list(state_dict.keys())[0].startswith('module.'):
            state_dict = {k[7:]: v for k, v in state_dict.items()}
        
        # å¤„ç†æ¨¡å‹é”®åä¸åŒ¹é…çš„é—®é¢˜
        if list(state_dict.keys())[0].startswith('model.'):
            state_dict = {k[6:]: v for k, v in state_dict.items()}
        
        self.model.load_state_dict(state_dict, strict=False)
    
    def preprocess_image(self, image_path):
        """
        å›¾åƒé¢„å¤„ç†
        Args:
            image_path: å›¾åƒè·¯å¾„
        Returns:
            tensor: é¢„å¤„ç†åçš„å›¾åƒå¼ é‡
            original_size: åŸå§‹å›¾åƒå°ºå¯¸ (height, width)
        """
        # è¯»å–å›¾åƒ
        image = cv2.imread(image_path, cv2.IMREAD_COLOR)
        if image is None:
            raise ValueError(f"æ— æ³•è¯»å–å›¾åƒ: {image_path}")
        
        original_size = image.shape[:2]  # (height, width)
        
        # BGRè½¬RGB
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # è°ƒæ•´å°ºå¯¸
        image = cv2.resize(image, (self.input_size[1], self.input_size[0]), 
                          interpolation=cv2.INTER_LINEAR)
        
        # å½’ä¸€åŒ–
        image = image.astype(np.float32) / 255.0
        image = (image - self.mean) / self.std
        
        # è½¬æ¢ä¸ºå¼ é‡
        image = torch.from_numpy(image.transpose(2, 0, 1)).float()
        image = image.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        return image, original_size
    
    def postprocess_prediction(self, pred, original_size):
        """
        åå¤„ç†é¢„æµ‹ç»“æœ
        Args:
            pred: æ¨¡å‹é¢„æµ‹è¾“å‡º
            original_size: åŸå§‹å›¾åƒå°ºå¯¸
        Returns:
            mask: åˆ†å‰²æ©ç  (numpy array)
            colored_mask: å½©è‰²åˆ†å‰²æ©ç 
        """
        # å¦‚æœæ¨¡å‹æœ‰å¤šä¸ªè¾“å‡ºï¼Œå–ç¬¬ä¸€ä¸ª
        if isinstance(pred, list):
            pred = pred[0]
        
        # è°ƒæ•´åˆ°åŸå§‹å°ºå¯¸
        pred = F.interpolate(pred, size=original_size, 
                           mode='bilinear', align_corners=False)
        
        # è½¬æ¢ä¸ºnumpyå¹¶è·å–ç±»åˆ«é¢„æµ‹
        pred = pred.cpu().numpy()[0]  # ç§»é™¤batchç»´åº¦
        mask = np.argmax(pred, axis=0)  # è·å–ç±»åˆ«ç´¢å¼•
        
        # åˆ›å»ºå½©è‰²æ©ç 
        colored_mask = np.zeros((original_size[0], original_size[1], 3), dtype=np.uint8)
        for class_id, color in enumerate(self.class_colors):
            colored_mask[mask == class_id] = color
        
        return mask, colored_mask
    
    def inference(self, image_path, save_result=True, output_dir='inference_results'):
        """
        å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œæ¨ç†
        Args:
            image_path: è¾“å…¥å›¾åƒè·¯å¾„
            save_result: æ˜¯å¦ä¿å­˜ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
        Returns:
            dict: åŒ…å«æ¨ç†ç»“æœçš„å­—å…¸
        """
        print(f"ğŸ” å¼€å§‹æ¨ç†: {image_path}")
        
        # é¢„å¤„ç†
        start_time = time.time()
        image_tensor, original_size = self.preprocess_image(image_path)
        image_tensor = image_tensor.to(self.device)
        preprocess_time = time.time() - start_time
        
        # æ¨ç†
        start_time = time.time()
        with torch.no_grad():
            pred = self.model(image_tensor)
        inference_time = time.time() - start_time
        
        # åå¤„ç†
        start_time = time.time()
        mask, colored_mask = self.postprocess_prediction(pred, original_size)
        postprocess_time = time.time() - start_time
        
        # ç»Ÿè®¡å„ç±»åˆ«åƒç´ æ•°é‡
        unique, counts = np.unique(mask, return_counts=True)
        class_stats = {}
        total_pixels = mask.size
        
        for class_id, count in zip(unique, counts):
            if class_id < len(self.class_names):
                class_name = self.class_names[class_id]
                percentage = (count / total_pixels) * 100
                class_stats[class_name] = {
                    'pixels': int(count),
                    'percentage': round(percentage, 2)
                }
        
        # ä¿å­˜ç»“æœ
        if save_result:
            os.makedirs(output_dir, exist_ok=True)
            
            # è·å–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            base_name = os.path.splitext(os.path.basename(image_path))[0]
            
            # ä¿å­˜åˆ†å‰²æ©ç 
            mask_path = os.path.join(output_dir, f"{base_name}_mask.png")
            cv2.imwrite(mask_path, mask.astype(np.uint8))
            
            # ä¿å­˜å½©è‰²æ©ç 
            colored_path = os.path.join(output_dir, f"{base_name}_colored.png")
            colored_mask_bgr = cv2.cvtColor(colored_mask, cv2.COLOR_RGB2BGR)
            cv2.imwrite(colored_path, colored_mask_bgr)
            
            # ä¿å­˜å åŠ å›¾åƒ
            original_image = cv2.imread(image_path)
            overlay = cv2.addWeighted(original_image, 0.7, colored_mask_bgr, 0.3, 0)
            overlay_path = os.path.join(output_dir, f"{base_name}_overlay.png")
            cv2.imwrite(overlay_path, overlay)
            
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
            print(f"   - åˆ†å‰²æ©ç : {mask_path}")
            print(f"   - å½©è‰²æ©ç : {colored_path}")
            print(f"   - å åŠ å›¾åƒ: {overlay_path}")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"â±ï¸ æ¨ç†æ—¶é—´:")
        print(f"   - é¢„å¤„ç†: {preprocess_time:.4f}s")
        print(f"   - æ¨¡å‹æ¨ç†: {inference_time:.4f}s")
        print(f"   - åå¤„ç†: {postprocess_time:.4f}s")
        print(f"   - æ€»æ—¶é—´: {preprocess_time + inference_time + postprocess_time:.4f}s")
        
        print(f"ğŸ“Š åˆ†å‰²ç»Ÿè®¡:")
        for class_name, stats in class_stats.items():
            print(f"   - {class_name}: {stats['pixels']} åƒç´  ({stats['percentage']}%)")
        
        return {
            'mask': mask,
            'colored_mask': colored_mask,
            'class_stats': class_stats,
            'timing': {
                'preprocess': preprocess_time,
                'inference': inference_time,
                'postprocess': postprocess_time,
                'total': preprocess_time + inference_time + postprocess_time
            }
        }

def parse_args():
    parser = argparse.ArgumentParser(description='å•å¼ å›¾ç‰‡è”æåˆ†å‰²æ¨ç†')
    
    parser.add_argument('--image', '-i',
                        required=True,
                        help='è¾“å…¥å›¾åƒè·¯å¾„')
    
    parser.add_argument('--config', '-c',
                        default='experiments/litchi/ddrnet23_litchi.yaml',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--model', '-m',
                        default='pth/best_val.pth',
                        help='æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--output', '-o',
                        default='inference_results',
                        help='è¾“å‡ºç›®å½•')
    
    parser.add_argument('--no-save',
                        action='store_true',
                        help='ä¸ä¿å­˜ç»“æœæ–‡ä»¶')
    
    return parser.parse_args()

def main():
    args = parse_args()
    
    print("ğŸš€ è”æåˆ†å‰²æ¨ç†å·¥å…·")
    print("=" * 50)
    
    try:
        # åˆ›å»ºæ¨ç†å™¨
        inferencer = LitchiInference(args.config, args.model)
        
        # æ‰§è¡Œæ¨ç†
        result = inferencer.inference(
            image_path=args.image,
            save_result=not args.no_save,
            output_dir=args.output
        )
        
        print("âœ… æ¨ç†å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {str(e)}")
        return 1
    
    return 0

if __name__ == '__main__':
    exit(main())